{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path1 = \"../Data/features_30_sec.csv\"\n",
    "path = '../Data/images_original/'\n",
    "subfolders = os.listdir(path)\n",
    "subfolders.remove('.DS_Store')\n",
    "path1 = '../Data/features_30_sec.csv'\n",
    "dataset = pd.read_csv(path1)\n",
    "columns = ['filename','label']\n",
    "features = dataset.drop(columns=columns, axis=1)  # Drop the 'label' column\n",
    "labels = dataset['label']\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "# Initialize a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform the training features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing features using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Initialize a label encoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the label encoder on the training labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Perform the same label encoding on the testing labels\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the logistic regression model with L1 regularization\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train_encoded)\n",
    "# Get the non-zero coefficients and corresponding feature names\n",
    "non_zero_coefficients = model.coef_[0]\n",
    "feature_names = features.columns\n",
    "\n",
    "# Create a dictionary to store the feature contributions\n",
    "feature_contributions = {}\n",
    "\n",
    "# Iterate over the non-zero coefficients and feature names\n",
    "for feature, coefficient in zip(feature_names, non_zero_coefficients):\n",
    "    feature_contributions[feature] = coefficient\n",
    "\n",
    "# Print the feature contributions in descending order\n",
    "sorted_contributions = sorted(feature_contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "negative_contributions = {feature: contribution for feature, contribution in feature_contributions.items() if contribution < 0}\n",
    "features_with_positive_contributions = features.drop(negative_contributions.keys(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_with_positive_contributions\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the features\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator GaussianNB from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model1 = joblib.load('nbc_model1.h5')\n",
    "model2 = joblib.load('nbc_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zeerakzubair/Documents/University/7th Semester/Deep Learning/DL_project/NBC/comparison.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zeerakzubair/Documents/University/7th%20Semester/Deep%20Learning/DL_project/NBC/comparison.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_predictions \u001b[39m=\u001b[39m model2\u001b[39m.\u001b[39mpredict(X_test_normalized)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zeerakzubair/Documents/University/7th%20Semester/Deep%20Learning/DL_project/NBC/comparison.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Generate the classification report\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zeerakzubair/Documents/University/7th%20Semester/Deep%20Learning/DL_project/NBC/comparison.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m classification_rep \u001b[39m=\u001b[39m classification_report(y_test_encoded, test_predictions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zeerakzubair/Documents/University/7th%20Semester/Deep%20Learning/DL_project/NBC/comparison.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Print the classification report\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zeerakzubair/Documents/University/7th%20Semester/Deep%20Learning/DL_project/NBC/comparison.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClassification: \u001b[39m\u001b[39m\"\u001b[39m,classification_rep)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the predictions on the test set\n",
    "test_predictions = model2.predict(X_test_normalized)\n",
    "\n",
    "# Generate the classification report\n",
    "classification_rep = classification_report(y_test_encoded, test_predictions)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification: \",classification_rep)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, test_predictions)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
